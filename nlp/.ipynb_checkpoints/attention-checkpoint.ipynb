{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Based on http://www.peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic attention\n",
    "\n",
    "The attention mechanism is just a sequence to sequence function. Given a sequence of vector inputs $x_1, x_2,\\ldots, x_t \\in \\mathbb{R}^d$, the attention mechanism produces an output sequence of vector inputs $y_1, y_2, \\ldots, y_t \\in \\mathbb{R}^d$ by doing the following:\n",
    "$$y_i = \\sum_{j} w_{ij} x_j.$$\n",
    "where\n",
    "$$w_{ij} = f(x_i, x_j).$$\n",
    "Often, $f(x_i, x_j)$ is taken to be $x_i^Tx_j$. In this case, $y_i$ is the weighted sum of all $x_1,\\ldots, x_t$ weighted by the similarity of $x_i$ and all $x_1,\\ldots x_t$. Also, it is conventional to normalize the weights by a softmax: $w_{ij} \\leftarrow \\frac{\\exp(w_{ij})}{\\sum_{j} \\exp(w_{ij})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of basic attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def basic_attention(x):\n",
    "    w = x.matmul(x.transpose(-1, -2))\n",
    "    normalized_w = F.softmax(w, dim = -1)\n",
    "    y = torch.matmul(normalized_w, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2173,  2.4251],\n",
      "        [-0.6028, -0.6962],\n",
      "        [ 2.3391,  1.3497],\n",
      "        [-0.9528, -1.0761]])\n"
     ]
    }
   ],
   "source": [
    "t, d = 4, 2\n",
    "x = torch.randn(t, d)\n",
    "y = basic_attention(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that attention is permuation equivariant. If you permute the input, then the output is permuted in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2173,  2.4251],\n",
      "        [-0.6028, -0.6962],\n",
      "        [-0.9528, -1.0761],\n",
      "        [ 2.3391,  1.3497]])\n"
     ]
    }
   ],
   "source": [
    "perm = torch.LongTensor([0, 1, 3, 2])\n",
    "x_new = x[perm]\n",
    "y_new = basic_attention(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the last two vectors are permuted in the input and the output of attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Query, key, value\n",
    " \n",
    " Notice that the basic attention mechanism has no tunable parameters. We can give it some tunable parameters by noticing that each input vector $x_i$ appears three times in the computation:\n",
    " 1. Computing the weights $x_i^Tx_j$ for all $j$\n",
    " 2. Computing the weights $x_j^Tx_i$ for all $j$\n",
    " 3. Acting as a basis vector $w_{i'i} x_i$\n",
    "Thus we can introduce three linear maps $W_q, W_k, W_v: \\mathbb{R}^d \\to \\mathbb{R}^d$ to compute the query, key, and value such that \n",
    "$$q_i = W_q x_i$$\n",
    "$$k_i = W_k x_i$$\n",
    "$$v_i = W_v x_i$$\n",
    "$$w'_{ij} = q_i^T k_j$$\n",
    "$$w_{ij} \\leftarrow softmax(w'_{i1}, \\ldots, w'_{it})$$\n",
    "$$y_i = \\sum_{j} w_{ij} v_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq = torch.randn(d, d)\n",
    "wk = torch.randn(d, d)\n",
    "wv = torch.randn(d, d)\n",
    "def qkv_attention(x):\n",
    "    d = x.size(-1)\n",
    "    q = x.matmul(wq)\n",
    "    k = x.matmul(wk)\n",
    "    v = x.matmul(wv)\n",
    "    w = q.matmul(k.transpose(-1, -2))\n",
    "    y = w.matmul(v)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that attention was permutation equivariant. In fact, even with query, keys, and values, it is still equivariant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.9639,  22.1648],\n",
      "        [ -2.6682, -11.1776],\n",
      "        [  1.1420,   1.5704],\n",
      "        [  2.7934,  17.0057]])\n"
     ]
    }
   ],
   "source": [
    "t, d = 4, 2\n",
    "x = torch.randn(t, d)\n",
    "y = qkv_attention(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.9639,  22.1649],\n",
      "        [ -2.6682, -11.1776],\n",
      "        [  2.7934,  17.0057],\n",
      "        [  1.1420,   1.5704]])\n"
     ]
    }
   ],
   "source": [
    "perm = torch.LongTensor([0, 1, 3, 2])\n",
    "x_new = x[perm]\n",
    "y_new = qkv_attention(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the inner product\n",
    "\n",
    "The softmax can be very sensitive to extreme values due to the exponentiation. This can cause vanishing gradients for the non-outliers.\n",
    "Since the expected length of random normal vectors in $d$ dimensions is roughly $\\sqrt d$, we rescale the inner products before the softmax:\n",
    "$$w'_{ij} = \\frac{q_i^T k_j}{\\sqrt{d}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "\n",
    "We can introduce multi-head attention which creates $h$ query, key, and value linear maps and concatenate all the $y_i^r$ before reducing the dimension back down to $d$.\n",
    "$$q_i^r = W_q^r x_i$$\n",
    "$$k_i^r = W_k^r x_i$$\n",
    "$$v_i^r = W_v^r x_i$$\n",
    "$$w_{ij}^{'r} = q_i^{rT} k_j^r$$\n",
    "$$w_{ij}^r \\leftarrow softmax(w_{i1}^{'r}, \\ldots, w_{it}^{'r})$$\n",
    "$$y_i^r = \\sum_{j} w_{ij}^r v_j^r$$\n",
    "$$y_i = W stack(y_i^1, \\ldots, y_i^h)$$\n",
    "where $W\\in \\mathbb{R}^{d\\times hd}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
